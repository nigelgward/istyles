\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\oddsidemargin 1mm

\textwidth 6.3in 
\topmargin -5mm
\headheight 5mm 
\headsep 8mm
\textheight 8.9in

\renewcommand{\floatpagefraction}{.8}
\renewcommand{\baselinestretch}{1.0}
\parindent 0pt
\parskip  4pt

%%=========================================================
\begin{document}
\noindent
\thispagestyle{empty}
\sloppy

\rule{1mm}{0mm}

\vspace{-17mm}
{\LARGE \bf Interaction Style Modeling  Toolset }

\smallskip
{\large \bf an extension of the  Midlevel Prosodic Features Toolikit}
\medskip


{\LARGE \bf Version 1.0}
\vspace{7mm}


{\bf Nigel Ward, University of Texas at El Paso}

{\bf \today }
\bigskip

%\vspace{-1ex}
%\begin{tabular}{p{7cm}rl}
%  & \ref{sec:overview} & Overview  \\
%  & \ref{sec:starting} & Getting the Code 
%\end{tabular}

\vspace{-3.5ex}
%%=========================================================
\section{Overview}    \label{sec:overview}

This toolset supports the analysis of interaction styles in spoken
dialog.  To quote from the abstract of  my as-yet-published paper:

\begin{quote}
  In spoken dialog, people clearly vary in their interaction styles,
  but a comprehensive model of the space of variation has been
  lacking.  To address this need, I applied Principal Component
  Analysis, using features designed to capture aspects of
  interaction-related prosodic behaviors, to tens of thousands of conversation
  fragments from the Switchboard corpus of American English telephone
  speech. 
\end{quote}


This toolset has so far been used only for the analyses reported in
the paper, but is ddesigned to generally support computational
analyses of interaction styles, for both scientific and practical
purposes, as discussed in the paper.

Specifically, this code enables one to:

\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item derive a vector-space representation of interaction styles from
  a corpus of stereo-recorded spoken dialogs.
\item given any new dialog, compute
  where it lies in an existing space of interaction styles.
\item given a corpus with metadata (for now just  Switchboard), compute various
  correlations and statistics on the factors affecting interaction styles .
\end{itemize}

This document serves mostly to overview the workflow and name the
specific Matlab functions to call.  It should be read after getting
the big picture from the paper, and can be followed by reading the
comments in the code. It is a work in progress; comments and
suggestions are welcome.


%==========================================================
\section{Getting the Code}  \label{sec:starting}

This code was written in Matlab and runs on Matlab version 2019.

The code is at {\tt https://github.com/nigelgward/TBD }.  It requires
%\begin{itemize}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
the Midlevel Toolkit, which is available at 
{\tt https://github.com/nigelgward/midlevel/ }, complete with documentation. 

%==========================================================
\section{Terminology}

The term ``dimension'' in the comments may refer either to a prosodic
construction dimension or to an interaction style dimensions.

%==========================================================
\section{Corpus Preparation }

Find a disk with 60 GB available, create a directory, refered to below
as {\tt motherDirectory}, and copy the Switchboard data discs there
one by one, naming the subdirectories {\tt disc1} \ldots {\tt disc4}.

Create {\tt wav}-format copies of each audio file using {\tt
  code/sph-to-wav.sh} .

Create the pitch files using {\tt midlevel/src/reaperize.sh} .

Decide how to split the data into subsets, and in {\it splits/} create
an index file for each. How I did this for Switchboard is described in
{\tt labnotes.txt}.  Each index file contains a one-per-line listing
of the audio files in that set.

Run {\tt prepMetadata.sh}.

%==========================================================
\section{Building the Model}

Open Matlab and change to the {\tt code} subdirectory (since that's
where {\tt rotationspec.mat} is found).

Run {\tt computeStyleParams.m('splits/trainset.txt', 'trainStats.csv')}
to create the features for all files in the training set.  The output,
here {\tt trainStats.csv}, is sometimes called a PCB file, as it
contains features based on prosodic-construction bin counts.

This will time (12 hours on a 8GB 3.6GHz machine), so if it crashes,
be prepared to look at {\tt trainStats.csv} to see how far it got, and
edit the code to just process the conversations not already processed;
the function will conveniently append stats for those new
conversations.

The {\tt computeStyleParams} function has three hidden dependencies,
in which it reads data from hardcoded filenames.  The first two, {\tt
  fsfile} and {\tt rsfile} specify how to compute the prosodic
dimensions, as described in the midlevel documentation. The third is
{\tt sifile}, listing the standard deviations of the prosodic
dimensions, as derived from the pbook data, and is used to determine
the ranges of the bins. Usually this should be held constant for all
analyses, unless you want to define not only a new interaction space
but also to redefine the basic features used.

\medskip
Now change to the {\tt stats} subdirectory and run {\tt
  deriveISspace('trainStats.csv', true, 'trainIsNormRot.mat',
  'trainIStyles')} (or {\tt 'train-30sec.csv'}).  This will create the
model and save its parameters in the {\tt mat} file. It will also
print out many interesting statistics that can be copied into the
paper.  It also pops up a figure that will go in the paper (when {\tt
  compareWithSubsets} is uncommented).

This will also write several files, notably:

\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item {\tt loadingsTables.txt}, written to {\tt stats}, shows, for each
  dimension (each interaction-style dimension) the loadings on the
  behavioral features (the prosodic-constructions-binned features).
\item many files with names starting with {\tt isratios}.  These can
  be examined, for example by {\tt sort -n isratios1hi.txt | more}.
  Once computed, it's nice to save them in a safe place, for example
  by moving them all to {\tt wordstats}. 
\item many files with names starting with {\tt idimWords}.  These can
  be examined with LIWC.  They also should be moved to {\tt wordstats}.
\item {\tt sox-commands.sh}, a script which can be run with {\tt bash}
  to generate various audio fragments.  These can then be listened to
  to help understand the meaning of the various dimensions.  The
  script also creates some anchor stimuli, to use in the
  human-perceptions experiment.  Once created, it's nice to move these
  all somewhere safe, e.g. {\tt exemplars}.
\end{itemize}

%==========================================================
\section{Applying a Model to New Data}

This is to be done, for example, when choosing clips to use for the
experiment, which should, of course, be taken from data not used in
training or interpreting the dimensions: thus from held out testset
data.

In {\tt code} run {\tt computeStyleParams.m('splits/testset2.txt',
  'test2Stats.csv')}.  This will take  3--4 hours.

** not tested yet **

In {\tt stats} run {\tt deriveISspace('test2Stats.csv', false,
  'trainIsNormRot.mat', 'testIStyles')}, importantly specifying {\tt false} for the
second parameter.

This will not change the model, but it will apply it to the testset
data, most usefully generating a {\tt sox-commands.sh} file to use to
generate fragments to use as stimuli, and the predicted scores to
compare to the turker-assigned stores. 


%==========================================================
\section{Acknowledgments }  


%%=========================================================
\bibliographystyle{IEEEtran}
%\bibliography{../../book/bib}
\bibliography{bib}     % temporary local copy 

%%=========================================================
\end{document}
